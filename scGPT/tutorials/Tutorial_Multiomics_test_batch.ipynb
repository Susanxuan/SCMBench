{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944c58ab",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Model for Multiomic Integration\n",
    "In this tutorial, we demonstrate how to fine-tune a pre-trained model for multiomic integration. We use the BMMC dataset as an example for RNA and protein abundance data integration from CITE-seq. The BMMC dataset consists of multiple batches by donor origin. In the paired integration setting, each cell contains both RNA and protein measurements. We fine-tune on the pre-trained whole-body model.\n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on integration tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190fb7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/scanpy/_settings.py:450: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "#from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "# import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from scgpt import prepare_data, prepare_dataloader, define_wandb_metrcis, evaluate, eval_testdata, train, eval_testdata_save_embed\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.model import MultiOmicTransformerModel\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.tokenizer import random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3931b62",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for integration task\n",
    "Here we provide some hyper-parameter recommendations for the multiomic integration task. Note that the BMMC dataset contains multiple batches to be integrated. Therefore, in addition to the default gene modelling objectives, we also turn on DAR objectives specifically to faciliate batch integration. We also turn on the use_mod argument as default to ensure that the model is modality-aware during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e9da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    task = 'multiomic',\n",
    "    seed=42,\n",
    "    dataset_name='SCMBench', # Dataset name \"BMMC\"\n",
    "    do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "    load_model=\"../save/scGPT_human\", # Path to pre-trained model\n",
    "    freeze = False, #freeze\n",
    "    GEP=True, # Gene expression modelling\n",
    "    GEPC=True, # Gene expression modelling for cell objective\n",
    "    CLS=False,\n",
    "    ESC=False,\n",
    "    DAR = True, # DAR objective weight for batch correction\n",
    "    DSBN = False,  # Domain-spec batchnorm,\n",
    "    mask_ratio=0.4, # Default mask ratio\n",
    "    explicit_zero_prob = False,  # whether explicit bernoulli for zeros\n",
    "    ecs_thres=0,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0,\n",
    "    use_batch_labels = True,\n",
    "    use_mod = True,\n",
    "    per_seq_batch_sample = False,\n",
    "    epochs=25, # Default number of epochs for fine-tuning #25\n",
    "    input_layer_key = \"X_binned\", # Default expression value binning in data pre-processing\n",
    "    n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "    n_hvg = 1200,  # Default number of highly variable genes\n",
    "    n_hvp = 4000,\n",
    "    max_seq_len = 4001, # # Default n_hvg+1\n",
    "    lr=1e-3, # Default learning rate for fine-tuning\n",
    "    batch_size=16, # Default batch size for fine-tuning\n",
    "    layer_size=512,\n",
    "    nlayers=4,\n",
    "    nhead=8, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "    schedule_ratio=0.95,  # Default rate for learning rate decay\n",
    "    save_eval_interval=5, # Default model evaluation interval\n",
    "    log_interval=100, # Default log interval\n",
    "    fast_transformer=True, # Default setting\n",
    "    pre_norm=False, # Default setting\n",
    "    amp=True,  # Default setting: Automatic Mixed Precision\n",
    "    pad_token = \"<pad>\",\n",
    "    mask_value = -1,\n",
    "    pad_value = -2,\n",
    "    include_zero_gene = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e7e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m18763105182\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/nas/user/yixuan/scGPT/tutorials/wandb/run-20240117_110307-dyklu2q1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/18763105182/scGPT/runs/dyklu2q1' target=\"_blank\">clear-wood-51</a></strong> to <a href='https://wandb.ai/18763105182/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/18763105182/scGPT' target=\"_blank\">https://wandb.ai/18763105182/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/18763105182/scGPT/runs/dyklu2q1' target=\"_blank\">https://wandb.ai/18763105182/scGPT/runs/dyklu2q1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'multiomic', 'seed': 42, 'dataset_name': 'SCMBench', 'do_train': True, 'load_model': '../save/scGPT_human', 'freeze': False, 'GEP': True, 'GEPC': True, 'CLS': False, 'ESC': False, 'DAR': True, 'DSBN': False, 'mask_ratio': 0.4, 'explicit_zero_prob': False, 'ecs_thres': 0, 'dab_weight': 1.0, 'use_batch_labels': True, 'use_mod': True, 'per_seq_batch_sample': False, 'epochs': 25, 'input_layer_key': 'X_binned', 'n_bins': 51, 'n_hvg': 1200, 'n_hvp': 4000, 'max_seq_len': 4001, 'lr': 0.001, 'batch_size': 16, 'layer_size': 512, 'nlayers': 4, 'nhead': 8, 'dropout': 0.2, 'schedule_ratio': 0.95, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'pad_token': '<pad>', 'mask_value': -1, 'pad_value': -2, 'include_zero_gene': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327a9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "special_tokens = [config.pad_token, \"<cls>\", \"<eoc>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf30635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_SCMBench-Jan17-11-03\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f7516",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653e6b7",
   "metadata": {},
   "source": [
    "### 2.1 Load the BMMC data\n",
    "Please download the data from https://drive.google.com/file/d/10RxboePS5p2Jj2Sfq1Ghzgqnl6nqPv5V/view?usp=sharing\n",
    "\n",
    "For the purpose of this tutorial, we selected a 12K+ cell subset from the BMMC dataset (originally 90K+ cells) to speed up training. More specifically, we filtered on the first 3 donors (batches) with B, Mono and T cell subtypes as shown in the data loading step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469efeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir_name='10x-Multiome-Pbmc10k-small'\n",
    "# dataset_dir_name='Chen-2019-small'\n",
    "# dataset_dir_name='Ma-2020-batch-53-small'\n",
    "dataset_dir_name='Ma-2020-small'\n",
    "input_rna_dir='/mnt/nas/user/yixuan/Multiomics-benchmark-main/data/download/'+dataset_dir_name+'/'+dataset_dir_name+'-RNA.h5ad'\n",
    "input_atac_dir='/mnt/nas/user/yixuan/Multiomics-benchmark-main/data/download/'+dataset_dir_name+'/'+dataset_dir_name+'-ATAC.h5ad'\n",
    "\n",
    "output_rna_dir='/mnt/nas/user/yixuan/Multiomics-benchmark-main/evaluation/workflow/scripts/scGPT-output/'+dataset_dir_name+'/'+dataset_dir_name+'-rna.csv'\n",
    "output_atac_dir='/mnt/nas/user/yixuan/Multiomics-benchmark-main/evaluation/workflow/scripts/scGPT-output/'+dataset_dir_name+'/'+dataset_dir_name+'-atac.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55e4231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/Multiomics-benchmark-main/data/download/Ma-2020-small/Ma-2020-small-RNA.h5ad\n",
      "/mnt/nas/user/yixuan/Multiomics-benchmark-main/evaluation/workflow/scripts/scGPT-output/Ma-2020-small/Ma-2020-small-rna.csv\n"
     ]
    }
   ],
   "source": [
    "print(input_rna_dir)\n",
    "print(output_rna_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f8edf6-4c5d-4056-8954-c11c737a50a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n"
     ]
    }
   ],
   "source": [
    "if dataset_name == 'BMMC':\n",
    "    adata = sc.read('scmdata/BMMC_processed.h5ad')\n",
    "    # subset to first 3 donors with B, Mono and T cell subtypes\n",
    "    adata = adata[adata.obs.DonorID.isin([10886, 11466, 12710]) & adata.obs.cell_type.isin(np.unique(adata.obs.cell_type.values)[:17])]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(str).astype('category')\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    encoded_batch = le.fit_transform(adata.obs['batch'].values)\n",
    "    adata.obs[\"batch_id\"] =  encoded_batch\n",
    "    adata.obs[\"str_batch\"] = adata.obs[\"batch_id\"].astype('category')\n",
    "    adata_protein = adata[:, adata.var.feature_types.isin(['ADT'])].copy()\n",
    "    adata_protein.var.index = ['p_' + i for i in adata_protein.var.index]\n",
    "    adata = adata[:, adata.var.feature_types.isin(['GEX'])].copy()\n",
    "    data_is_raw = False\n",
    "if dataset_name == 'SCMBench':\n",
    "    adata = anndata.read_h5ad(input_rna_dir)\n",
    "    adata_protein = anndata.read_h5ad(input_atac_dir)\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(str).astype('category')\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    encoded_batch = le.fit_transform(adata.obs['batch'].values)\n",
    "    adata.obs[\"batch_id\"] =  encoded_batch\n",
    "    adata.obs[\"str_batch\"] = adata.obs[\"batch_id\"].astype('category')\n",
    "    data_is_raw = False\n",
    "    print('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae0a311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>protocol</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>batch</th>\n",
       "      <th>celltype</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>str_batch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cells</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R1.01.R2.01.R3.06.P1.55</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>55</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.01.R2.03.R3.68.P1.55</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>55</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.01.R2.05.R3.15.P1.53</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>53</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.01.R2.05.R3.40.P1.55</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>55</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.01.R2.05.R3.49.P1.55</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>55</td>\n",
       "      <td>Dermal Fibroblast</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.92.R2.79.R3.05.P1.56</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>56</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.93.R2.20.R3.18.P1.53</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>53</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.93.R2.80.R3.62.P1.55</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>55</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.93.R2.91.R3.82.P1.56</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>56</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1.94.R2.75.R3.40.P1.55</th>\n",
       "      <td>scRNA-seq</td>\n",
       "      <td>SHARE-seq</td>\n",
       "      <td>Ma-2020-RNA</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>55</td>\n",
       "      <td>Melanocyte</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32231 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            domain   protocol      dataset          cell_type  \\\n",
       "cells                                                                           \n",
       "R1.01.R2.01.R3.06.P1.55  scRNA-seq  SHARE-seq  Ma-2020-RNA  Dermal Fibroblast   \n",
       "R1.01.R2.03.R3.68.P1.55  scRNA-seq  SHARE-seq  Ma-2020-RNA  Dermal Fibroblast   \n",
       "R1.01.R2.05.R3.15.P1.53  scRNA-seq  SHARE-seq  Ma-2020-RNA  Dermal Fibroblast   \n",
       "R1.01.R2.05.R3.40.P1.55  scRNA-seq  SHARE-seq  Ma-2020-RNA  Dermal Fibroblast   \n",
       "R1.01.R2.05.R3.49.P1.55  scRNA-seq  SHARE-seq  Ma-2020-RNA  Dermal Fibroblast   \n",
       "...                            ...        ...          ...                ...   \n",
       "R1.92.R2.79.R3.05.P1.56  scRNA-seq  SHARE-seq  Ma-2020-RNA         Melanocyte   \n",
       "R1.93.R2.20.R3.18.P1.53  scRNA-seq  SHARE-seq  Ma-2020-RNA         Melanocyte   \n",
       "R1.93.R2.80.R3.62.P1.55  scRNA-seq  SHARE-seq  Ma-2020-RNA         Melanocyte   \n",
       "R1.93.R2.91.R3.82.P1.56  scRNA-seq  SHARE-seq  Ma-2020-RNA         Melanocyte   \n",
       "R1.94.R2.75.R3.40.P1.55  scRNA-seq  SHARE-seq  Ma-2020-RNA         Melanocyte   \n",
       "\n",
       "                        batch           celltype  batch_id str_batch  \n",
       "cells                                                                 \n",
       "R1.01.R2.01.R3.06.P1.55    55  Dermal Fibroblast         2         2  \n",
       "R1.01.R2.03.R3.68.P1.55    55  Dermal Fibroblast         2         2  \n",
       "R1.01.R2.05.R3.15.P1.53    53  Dermal Fibroblast         0         0  \n",
       "R1.01.R2.05.R3.40.P1.55    55  Dermal Fibroblast         2         2  \n",
       "R1.01.R2.05.R3.49.P1.55    55  Dermal Fibroblast         2         2  \n",
       "...                       ...                ...       ...       ...  \n",
       "R1.92.R2.79.R3.05.P1.56    56         Melanocyte         3         3  \n",
       "R1.93.R2.20.R3.18.P1.53    53         Melanocyte         0         0  \n",
       "R1.93.R2.80.R3.62.P1.55    55         Melanocyte         2         2  \n",
       "R1.93.R2.91.R3.82.P1.56    56         Melanocyte         3         3  \n",
       "R1.94.R2.75.R3.40.P1.55    55         Melanocyte         2         2  \n",
       "\n",
       "[32231 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bfe331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_mod:\n",
    "    gene_rna_df = pd.DataFrame(index = adata.var.index.tolist())\n",
    "    gene_rna_df['mod'] = 'RNA'\n",
    "    gene_protein_df = pd.DataFrame(index = adata_protein.var.index.tolist())\n",
    "    gene_protein_df['mod'] = 'Protein'\n",
    "    gene_loc_df = pd.concat([gene_rna_df, gene_protein_df])\n",
    "    gene_loc_df['mod'] = gene_loc_df['mod'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7371ebc",
   "metadata": {},
   "source": [
    "### 2.2 Cross-check gene set with the pre-trained model \n",
    "Note that we retain the common gene set between the data and the pre-trained model for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc7d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 10/8000 genes in vocabulary of size 60697.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    old_vocab = vocab\n",
    "embsize = config.layer_size\n",
    "nhead = config.nhead\n",
    "nlayers = config.nlayers\n",
    "d_hid = config.layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f414d",
   "metadata": {},
   "source": [
    "### 2.3 Pre-process the data\n",
    "We follow the standardized pipline of depth normalization, log normalization, and highly vairable gene (HVG) selection for data pre-processing. We further introduce value binning to obtain the relative expressions of each HVG. Given multiple sequencing modalities, we perform the pre-processing steps on each individual modality first, and then combine them into multi-modal sequences as model input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32228422",
   "metadata": {},
   "source": [
    "#### 2.3.1 Pre-process the RNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69be564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=1,  # step 1\n",
    "    filter_cell_by_counts=1,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes #config.n_hvg\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efe68e",
   "metadata": {},
   "source": [
    "#### 2.3.2 Pre-process the Protein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a72dc9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor_protein = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=0,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=False,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=False,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=None,\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor_protein(adata_protein, batch_key=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e11257",
   "metadata": {},
   "source": [
    "#### 2.3.3 Combine RNA, and Protein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab9f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = np.concatenate([adata.layers[\"X_binned\"], adata_protein.layers[\"X_binned\"]], axis=1)\n",
    "adata = AnnData(\n",
    "    X=data_combined,\n",
    "    obs=adata.obs,\n",
    "    var=pd.DataFrame(index=adata.var_names.tolist() + adata_protein.var_names.tolist()),\n",
    "    layers={\"X_binned\": data_combined,}\n",
    ")\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f717788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 32231 × 88000\n",
       "    obs: 'domain', 'protocol', 'dataset', 'cell_type', 'batch', 'celltype', 'batch_id', 'str_batch', 'n_counts'\n",
       "    var: 'gene_name'\n",
       "    layers: 'X_binned'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "372f68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c2a1c",
   "metadata": {},
   "source": [
    "### 2.4 Tokenize the input data for model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "116c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = (\n",
    "    adata.layers[config.input_layer_key].A\n",
    "    if issparse(adata.layers[config.input_layer_key])\n",
    "    else adata.layers[config.input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"cell_type\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ac9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_mod:\n",
    "    mod_type = np.array([gene_loc_df.loc[g, 'mod'] for g in genes])\n",
    "    vocab_mod = Vocab(VocabPybind(np.unique(gene_loc_df['mod']).tolist() + special_tokens, None))\n",
    "    vocab_mod.set_default_index(vocab_mod[\"<pad>\"])\n",
    "    mod_type = np.array(vocab_mod(list(mod_type)), dtype=int)\n",
    "    ntokens_mod = len(vocab_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753b6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a942b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max num of non_zero genes: 11281\n",
      "min num of non_zero genes: 147\n",
      "average num of non_zero genes: 875.0596063019271\n",
      "99% quantile num of non_zero genes: 3623.639999999992\n",
      "max original values: 50\n",
      "average original non_zero values: 25.264929388948932\n",
      "99% quantile original non_zero values: 49.0\n",
      "num of celltypes: 22\n"
     ]
    }
   ],
   "source": [
    "num_of_non_zero_genes = [\n",
    "    np.count_nonzero(train_data[i]) for i in range(train_data.shape[0])\n",
    "]\n",
    "print(f\"max num of non_zero genes: {np.max(num_of_non_zero_genes)}\")\n",
    "print(f\"min num of non_zero genes: {np.min(num_of_non_zero_genes)}\")\n",
    "print(f\"average num of non_zero genes: {np.mean(num_of_non_zero_genes)}\")\n",
    "print(\n",
    "    f\"99% quantile num of non_zero genes: {np.quantile(num_of_non_zero_genes, 0.99)}\"\n",
    ")\n",
    "print(f\"max original values: {np.max(train_data)}\")\n",
    "print(\n",
    "    f\"average original non_zero values: {np.mean(train_data[np.nonzero(train_data)])}\"\n",
    ")\n",
    "print(\n",
    "    f\"99% quantile original non_zero values: {np.quantile(train_data[np.nonzero(train_data)], 0.99)}\"\n",
    ")\n",
    "print(f\"num of celltypes: {num_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd4b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(VocabPybind(genes + special_tokens, None))\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "else:\n",
    "    pretrained_genes = [g for g in genes + special_tokens if g in old_vocab]\n",
    "    new_genes = [g for g in genes + special_tokens if g not in old_vocab]\n",
    "    gene_ids_pretrained = np.array(old_vocab(pretrained_genes), dtype=int)\n",
    "    # https://discuss.pytorch.org/t/expand-an-existing-embedding-and-linear-layer-nan-loss-value/55670/2\n",
    "    # Retrieve pretrained weights\n",
    "    vocab = Vocab(VocabPybind(pretrained_genes + new_genes, None))\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b578ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 29007, \n",
      "\t feature length: 4001\n",
      "scGPT - INFO - valid set number of samples: 3224, \n",
      "\t feature length: 4001\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=config.max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=config.pad_token,\n",
    "    pad_value=config.pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=config.include_zero_gene,\n",
    "    mod_type=mod_type if config.use_mod else None,\n",
    "    vocab_mod=vocab_mod if config.use_mod else None,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=config.max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=config.pad_token,\n",
    "    pad_value=config.pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=config.include_zero_gene,\n",
    "    mod_type=mod_type if config.use_mod else None,\n",
    "    vocab_mod=vocab_mod if config.use_mod else None,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f78ec",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model\n",
    "Note that for multiomic integration, since the pre-trained model does not include the ATAC and protein tokens, we expand the embedding layer by adding these new tokens. We inherit only the gene embedding layer from the pre-trained model, and train rest of the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd642466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d57aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiOmicTransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(88003, 512, padding_idx=10)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): ContinuousValueEncoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (batch_encoder): BatchLabelEncoder(\n",
      "    (embedding): Embedding(4, 512)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (mod_encoder): BatchLabelEncoder(\n",
      "    (embedding): Embedding(5, 512, padding_idx=2)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ExprDecoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cls_decoder): ClsDecoder(\n",
      "    (_decoder): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (mvc_decoder): MVCDecoder(\n",
      "    (gene2query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (query_activation): Sigmoid()\n",
      "    (W): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  )\n",
      "  (grad_reverse_discriminator): AdversarialDiscriminator(\n",
      "    (_decoder): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (4): LeakyReLU(negative_slope=0.01)\n",
      "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_layer): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      "  (sim): Similarity(\n",
      "    (cos): CosineSimilarity()\n",
      "  )\n",
      "  (creterion_cce): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_number = 1\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{device_number}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model_dict = torch.load(model_file)\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = MultiOmicTransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid, \n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=config.pad_token,\n",
    "    pad_value=config.pad_value,\n",
    "    do_mvc=config.GEPC,\n",
    "    do_dab=config.DAR,\n",
    "    use_batch_labels=config.use_batch_labels,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    n_input_bins=config.n_bins,\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=config.explicit_zero_prob,\n",
    "    use_fast_transformer=config.fast_transformer,\n",
    "    pre_norm=config.pre_norm,\n",
    "    use_mod=config.use_mod,\n",
    "    ntokens_mod=ntokens_mod if config.use_mod else None,\n",
    "    vocab_mod=vocab_mod if config.use_mod else None,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    pretrained_emb_weights = model_dict['encoder.embedding.weight'][gene_ids_pretrained, :]\n",
    "    model.encoder.embedding.weight.data[:len(pretrained_genes), :] = pretrained_emb_weights\n",
    "    model.encoder.enc_norm.weight.data = model_dict['encoder.enc_norm.weight']\n",
    "ntokens = len(vocab)\n",
    "\n",
    "model.to(device)\n",
    "wandb.watch(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecff528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.GEP and config.GEPC:\n",
    "    criterion_gep_gepc = masked_mse_loss\n",
    "if config.CLS:\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "if config.DAR:\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d9e08",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62c9cc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   1 | 100/1813 batches | lr 0.00100 | ms/batch 171.52 | loss 10815.74 | gep 364.62 |gepc 10449.64 |dar  1.48 |\n",
      "scGPT - INFO - | epoch   1 | 200/1813 batches | lr 0.00100 | ms/batch 155.10 | loss 410.54 | gep 204.99 |gepc 204.12 |dar  1.42 |\n",
      "scGPT - INFO - | epoch   1 | 300/1813 batches | lr 0.00100 | ms/batch 152.95 | loss 408.27 | gep 204.79 |gepc 202.08 |dar  1.40 |\n",
      "scGPT - INFO - | epoch   1 | 400/1813 batches | lr 0.00100 | ms/batch 154.21 | loss 407.37 | gep 204.38 |gepc 201.63 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   1 | 500/1813 batches | lr 0.00100 | ms/batch 157.23 | loss 407.89 | gep 204.63 |gepc 201.90 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   1 | 600/1813 batches | lr 0.00100 | ms/batch 167.50 | loss 407.48 | gep 204.51 |gepc 201.60 |dar  1.37 |\n",
      "scGPT - INFO - | epoch   1 | 700/1813 batches | lr 0.00100 | ms/batch 148.87 | loss 407.85 | gep 204.87 |gepc 201.62 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   1 | 800/1813 batches | lr 0.00100 | ms/batch 151.64 | loss 407.76 | gep 204.66 |gepc 201.74 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   1 | 900/1813 batches | lr 0.00100 | ms/batch 152.88 | loss 407.29 | gep 204.46 |gepc 201.48 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   1 | 1000/1813 batches | lr 0.00100 | ms/batch 151.74 | loss 406.91 | gep 204.31 |gepc 201.25 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   1 | 1100/1813 batches | lr 0.00100 | ms/batch 152.43 | loss 406.79 | gep 204.10 |gepc 201.33 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   1 | 1200/1813 batches | lr 0.00100 | ms/batch 151.40 | loss 407.07 | gep 204.29 |gepc 201.42 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   1 | 1300/1813 batches | lr 0.00100 | ms/batch 156.24 | loss 407.21 | gep 204.63 |gepc 201.23 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   1 | 1400/1813 batches | lr 0.00100 | ms/batch 156.00 | loss 408.27 | gep 205.07 |gepc 201.85 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   1 | 1500/1813 batches | lr 0.00100 | ms/batch 156.50 | loss 406.65 | gep 204.22 |gepc 201.07 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   1 | 1600/1813 batches | lr 0.00100 | ms/batch 156.49 | loss 406.82 | gep 204.36 |gepc 201.11 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   1 | 1700/1813 batches | lr 0.00100 | ms/batch 211.84 | loss 405.77 | gep 203.77 |gepc 200.65 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   1 | 1800/1813 batches | lr 0.00100 | ms/batch 210.96 | loss 407.74 | gep 204.82 |gepc 201.57 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 312.69s | valid loss 204.0371 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 204.0371\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   2 | 100/1813 batches | lr 0.00095 | ms/batch 205.70 | loss 410.93 | gep 206.40 |gepc 203.18 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 200/1813 batches | lr 0.00095 | ms/batch 162.55 | loss 407.79 | gep 204.80 |gepc 201.66 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   2 | 300/1813 batches | lr 0.00095 | ms/batch 152.16 | loss 407.41 | gep 204.77 |gepc 201.29 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 400/1813 batches | lr 0.00095 | ms/batch 155.04 | loss 406.99 | gep 204.45 |gepc 201.20 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   2 | 500/1813 batches | lr 0.00095 | ms/batch 161.91 | loss 406.36 | gep 204.13 |gepc 200.88 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   2 | 600/1813 batches | lr 0.00095 | ms/batch 152.19 | loss 406.75 | gep 204.35 |gepc 201.04 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 700/1813 batches | lr 0.00095 | ms/batch 207.81 | loss 406.00 | gep 204.06 |gepc 200.59 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 800/1813 batches | lr 0.00095 | ms/batch 210.85 | loss 406.66 | gep 204.20 |gepc 201.11 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 900/1813 batches | lr 0.00095 | ms/batch 211.44 | loss 406.75 | gep 204.36 |gepc 201.04 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   2 | 1000/1813 batches | lr 0.00095 | ms/batch 162.41 | loss 407.06 | gep 204.42 |gepc 201.28 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   2 | 1100/1813 batches | lr 0.00095 | ms/batch 164.56 | loss 407.46 | gep 204.71 |gepc 201.39 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 1200/1813 batches | lr 0.00095 | ms/batch 152.75 | loss 406.78 | gep 204.36 |gepc 201.07 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   2 | 1300/1813 batches | lr 0.00095 | ms/batch 153.14 | loss 406.47 | gep 203.96 |gepc 201.15 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   2 | 1400/1813 batches | lr 0.00095 | ms/batch 153.83 | loss 405.02 | gep 202.47 |gepc 201.20 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 1500/1813 batches | lr 0.00095 | ms/batch 153.55 | loss 405.17 | gep 202.62 |gepc 201.19 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   2 | 1600/1813 batches | lr 0.00095 | ms/batch 155.75 | loss 407.02 | gep 204.67 |gepc 201.00 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 1700/1813 batches | lr 0.00095 | ms/batch 153.69 | loss 407.13 | gep 204.50 |gepc 201.28 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   2 | 1800/1813 batches | lr 0.00095 | ms/batch 156.16 | loss 406.45 | gep 204.22 |gepc 200.90 |dar  1.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 315.27s | valid loss 204.7616 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   3 | 100/1813 batches | lr 0.00090 | ms/batch 155.81 | loss 411.09 | gep 206.62 |gepc 203.10 |dar  1.37 |\n",
      "scGPT - INFO - | epoch   3 | 200/1813 batches | lr 0.00090 | ms/batch 151.66 | loss 406.09 | gep 204.13 |gepc 200.60 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   3 | 300/1813 batches | lr 0.00090 | ms/batch 153.41 | loss 405.44 | gep 203.71 |gepc 200.37 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   3 | 400/1813 batches | lr 0.00090 | ms/batch 151.58 | loss 406.88 | gep 204.52 |gepc 201.02 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   3 | 500/1813 batches | lr 0.00090 | ms/batch 151.76 | loss 406.26 | gep 204.12 |gepc 200.80 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   3 | 600/1813 batches | lr 0.00090 | ms/batch 150.57 | loss 407.65 | gep 204.78 |gepc 201.53 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   3 | 700/1813 batches | lr 0.00090 | ms/batch 152.77 | loss 406.40 | gep 204.26 |gepc 200.80 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   3 | 800/1813 batches | lr 0.00090 | ms/batch 156.25 | loss 406.56 | gep 204.20 |gepc 201.01 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   3 | 900/1813 batches | lr 0.00090 | ms/batch 151.18 | loss 407.00 | gep 204.52 |gepc 201.15 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   3 | 1000/1813 batches | lr 0.00090 | ms/batch 152.81 | loss 406.71 | gep 204.47 |gepc 200.90 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   3 | 1100/1813 batches | lr 0.00090 | ms/batch 154.19 | loss 406.72 | gep 204.28 |gepc 201.11 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   3 | 1200/1813 batches | lr 0.00090 | ms/batch 152.27 | loss 406.27 | gep 204.25 |gepc 200.66 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   3 | 1300/1813 batches | lr 0.00090 | ms/batch 156.16 | loss 406.20 | gep 204.07 |gepc 200.78 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   3 | 1400/1813 batches | lr 0.00090 | ms/batch 152.45 | loss 407.32 | gep 204.65 |gepc 201.33 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   3 | 1500/1813 batches | lr 0.00090 | ms/batch 152.75 | loss 406.61 | gep 204.34 |gepc 200.91 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   3 | 1600/1813 batches | lr 0.00090 | ms/batch 155.29 | loss 406.21 | gep 204.17 |gepc 200.70 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   3 | 1700/1813 batches | lr 0.00090 | ms/batch 152.32 | loss 406.70 | gep 204.30 |gepc 201.04 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   3 | 1800/1813 batches | lr 0.00090 | ms/batch 154.13 | loss 406.83 | gep 204.44 |gepc 201.03 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 287.40s | valid loss 203.8953 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 203.8953\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   4 | 100/1813 batches | lr 0.00086 | ms/batch 153.66 | loss 410.23 | gep 206.14 |gepc 202.73 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   4 | 200/1813 batches | lr 0.00086 | ms/batch 150.03 | loss 406.47 | gep 204.28 |gepc 200.84 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   4 | 300/1813 batches | lr 0.00086 | ms/batch 151.86 | loss 406.96 | gep 204.49 |gepc 201.11 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   4 | 400/1813 batches | lr 0.00086 | ms/batch 153.76 | loss 406.51 | gep 204.35 |gepc 200.81 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   4 | 500/1813 batches | lr 0.00086 | ms/batch 154.68 | loss 405.96 | gep 204.00 |gepc 200.62 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   4 | 600/1813 batches | lr 0.00086 | ms/batch 153.80 | loss 407.15 | gep 204.55 |gepc 201.25 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   4 | 700/1813 batches | lr 0.00086 | ms/batch 154.10 | loss 406.43 | gep 204.36 |gepc 200.74 |dar  1.33 |\n",
      "scGPT - INFO - | epoch   4 | 800/1813 batches | lr 0.00086 | ms/batch 151.36 | loss 406.00 | gep 204.04 |gepc 200.62 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   4 | 900/1813 batches | lr 0.00086 | ms/batch 153.22 | loss 406.36 | gep 204.29 |gepc 200.71 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   4 | 1000/1813 batches | lr 0.00086 | ms/batch 154.34 | loss 406.40 | gep 204.28 |gepc 200.76 |dar  1.37 |\n",
      "scGPT - INFO - | epoch   4 | 1100/1813 batches | lr 0.00086 | ms/batch 156.53 | loss 406.42 | gep 204.26 |gepc 200.82 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   4 | 1200/1813 batches | lr 0.00086 | ms/batch 154.06 | loss 406.72 | gep 204.36 |gepc 201.01 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   4 | 1300/1813 batches | lr 0.00086 | ms/batch 156.02 | loss 405.93 | gep 203.98 |gepc 200.61 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   4 | 1400/1813 batches | lr 0.00086 | ms/batch 154.12 | loss 406.23 | gep 204.19 |gepc 200.70 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   4 | 1500/1813 batches | lr 0.00086 | ms/batch 155.23 | loss 406.72 | gep 204.33 |gepc 201.04 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   4 | 1600/1813 batches | lr 0.00086 | ms/batch 155.48 | loss 406.45 | gep 204.33 |gepc 200.77 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   4 | 1700/1813 batches | lr 0.00086 | ms/batch 156.13 | loss 406.49 | gep 204.26 |gepc 200.89 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   4 | 1800/1813 batches | lr 0.00086 | ms/batch 154.25 | loss 406.84 | gep 204.63 |gepc 200.87 |dar  1.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 289.28s | valid loss 204.3260 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   5 | 100/1813 batches | lr 0.00081 | ms/batch 154.79 | loss 411.22 | gep 206.66 |gepc 203.21 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 200/1813 batches | lr 0.00081 | ms/batch 151.81 | loss 406.01 | gep 204.05 |gepc 200.60 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 300/1813 batches | lr 0.00081 | ms/batch 152.05 | loss 406.58 | gep 204.41 |gepc 200.81 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 400/1813 batches | lr 0.00081 | ms/batch 150.21 | loss 407.04 | gep 204.60 |gepc 201.10 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   5 | 500/1813 batches | lr 0.00081 | ms/batch 149.22 | loss 406.25 | gep 204.14 |gepc 200.76 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 600/1813 batches | lr 0.00081 | ms/batch 154.44 | loss 405.93 | gep 204.13 |gepc 200.44 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 700/1813 batches | lr 0.00081 | ms/batch 154.69 | loss 407.24 | gep 204.68 |gepc 201.21 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 800/1813 batches | lr 0.00081 | ms/batch 151.32 | loss 406.12 | gep 204.11 |gepc 200.66 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 900/1813 batches | lr 0.00081 | ms/batch 150.28 | loss 406.65 | gep 204.41 |gepc 200.89 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 1000/1813 batches | lr 0.00081 | ms/batch 148.49 | loss 405.91 | gep 203.99 |gepc 200.58 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   5 | 1100/1813 batches | lr 0.00081 | ms/batch 149.93 | loss 406.83 | gep 204.52 |gepc 200.98 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   5 | 1200/1813 batches | lr 0.00081 | ms/batch 189.95 | loss 405.77 | gep 204.06 |gepc 200.38 |dar  1.33 |\n",
      "scGPT - INFO - | epoch   5 | 1300/1813 batches | lr 0.00081 | ms/batch 194.33 | loss 405.56 | gep 203.88 |gepc 200.33 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   5 | 1400/1813 batches | lr 0.00081 | ms/batch 190.28 | loss 406.30 | gep 204.34 |gepc 200.61 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 1500/1813 batches | lr 0.00081 | ms/batch 156.02 | loss 405.74 | gep 204.00 |gepc 200.39 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 1600/1813 batches | lr 0.00081 | ms/batch 150.98 | loss 406.05 | gep 204.10 |gepc 200.60 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 1700/1813 batches | lr 0.00081 | ms/batch 152.38 | loss 406.66 | gep 204.42 |gepc 200.90 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   5 | 1800/1813 batches | lr 0.00081 | ms/batch 153.84 | loss 406.15 | gep 204.18 |gepc 200.62 |dar  1.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 297.48s | valid loss 204.5331 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Saving model to save/dev_SCMBench-Jan17-11-03\n",
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2015/2015 [01:50<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_embeddings (32231, 512)\n",
      "NMI...\n",
      "ARI...\n",
      "Silhouette score...\n",
      "PC regression...\n",
      "Graph connectivity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO -                                   0\n",
      "NMI_cluster/label          0.026786\n",
      "ARI_cluster/label          0.000516\n",
      "ASW_label                  0.462518\n",
      "ASW_label/batch            0.945129\n",
      "PCR_batch                  0.959961\n",
      "cell_cycle_conservation         NaN\n",
      "isolated_label_F1               NaN\n",
      "isolated_label_silhouette       NaN\n",
      "graph_conn                 0.025884\n",
      "kBET                            NaN\n",
      "iLISI                           NaN\n",
      "cLISI                           NaN\n",
      "hvg_overlap                     NaN\n",
      "trajectory                      NaN\n",
      "scGPT - INFO - Biological Conservation Metrics: \n",
      "ASW (cell-type): 0.4625, graph cLISI: nan, isolated label silhouette: nan, \n",
      "Batch Effect Removal Metrics: \n",
      "PCR_batch: 0.9600, ASW (batch): 0.9451, graph connectivity: 0.0259, graph iLISI: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   6, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   6 | 100/1813 batches | lr 0.00077 | ms/batch 154.46 | loss 410.18 | gep 206.37 |gepc 202.45 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   6 | 200/1813 batches | lr 0.00077 | ms/batch 149.57 | loss 406.14 | gep 204.22 |gepc 200.56 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 300/1813 batches | lr 0.00077 | ms/batch 150.33 | loss 406.46 | gep 204.34 |gepc 200.77 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   6 | 400/1813 batches | lr 0.00077 | ms/batch 152.83 | loss 405.86 | gep 204.06 |gepc 200.45 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 500/1813 batches | lr 0.00077 | ms/batch 152.86 | loss 405.77 | gep 203.97 |gepc 200.46 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 600/1813 batches | lr 0.00077 | ms/batch 151.18 | loss 406.86 | gep 204.53 |gepc 200.99 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   6 | 700/1813 batches | lr 0.00077 | ms/batch 154.32 | loss 405.85 | gep 204.10 |gepc 200.39 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 800/1813 batches | lr 0.00077 | ms/batch 152.46 | loss 405.68 | gep 204.08 |gepc 200.26 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 900/1813 batches | lr 0.00077 | ms/batch 151.62 | loss 405.71 | gep 203.94 |gepc 200.43 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   6 | 1000/1813 batches | lr 0.00077 | ms/batch 150.55 | loss 405.45 | gep 203.85 |gepc 200.26 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   6 | 1100/1813 batches | lr 0.00077 | ms/batch 152.47 | loss 405.70 | gep 203.98 |gepc 200.38 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 1200/1813 batches | lr 0.00077 | ms/batch 152.66 | loss 406.11 | gep 204.10 |gepc 200.67 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   6 | 1300/1813 batches | lr 0.00077 | ms/batch 154.47 | loss 406.13 | gep 204.10 |gepc 200.67 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   6 | 1400/1813 batches | lr 0.00077 | ms/batch 150.09 | loss 406.52 | gep 204.38 |gepc 200.79 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 1500/1813 batches | lr 0.00077 | ms/batch 153.70 | loss 406.25 | gep 204.30 |gepc 200.60 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 1600/1813 batches | lr 0.00077 | ms/batch 153.11 | loss 405.45 | gep 203.84 |gepc 200.26 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 1700/1813 batches | lr 0.00077 | ms/batch 150.30 | loss 406.82 | gep 204.53 |gepc 200.95 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   6 | 1800/1813 batches | lr 0.00077 | ms/batch 151.40 | loss 406.23 | gep 204.22 |gepc 200.66 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 285.18s | valid loss 204.0716 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   7 | 100/1813 batches | lr 0.00074 | ms/batch 152.69 | loss 410.60 | gep 206.54 |gepc 202.71 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 200/1813 batches | lr 0.00074 | ms/batch 160.36 | loss 406.19 | gep 204.23 |gepc 200.61 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 300/1813 batches | lr 0.00074 | ms/batch 150.07 | loss 406.09 | gep 204.20 |gepc 200.53 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   7 | 400/1813 batches | lr 0.00074 | ms/batch 151.16 | loss 406.19 | gep 204.33 |gepc 200.51 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   7 | 500/1813 batches | lr 0.00074 | ms/batch 148.98 | loss 407.21 | gep 204.78 |gepc 201.09 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   7 | 600/1813 batches | lr 0.00074 | ms/batch 151.44 | loss 405.99 | gep 204.16 |gepc 200.48 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   7 | 700/1813 batches | lr 0.00074 | ms/batch 152.15 | loss 406.17 | gep 204.26 |gepc 200.57 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   7 | 800/1813 batches | lr 0.00074 | ms/batch 150.65 | loss 405.70 | gep 204.04 |gepc 200.31 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 900/1813 batches | lr 0.00074 | ms/batch 150.88 | loss 406.62 | gep 204.41 |gepc 200.86 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 1000/1813 batches | lr 0.00074 | ms/batch 150.40 | loss 405.74 | gep 203.99 |gepc 200.40 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   7 | 1100/1813 batches | lr 0.00074 | ms/batch 152.25 | loss 406.57 | gep 204.40 |gepc 200.83 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 1200/1813 batches | lr 0.00074 | ms/batch 157.54 | loss 404.94 | gep 203.68 |gepc 199.91 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 1300/1813 batches | lr 0.00074 | ms/batch 150.48 | loss 405.67 | gep 203.92 |gepc 200.41 |dar  1.33 |\n",
      "scGPT - INFO - | epoch   7 | 1400/1813 batches | lr 0.00074 | ms/batch 152.62 | loss 405.80 | gep 203.97 |gepc 200.48 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 1500/1813 batches | lr 0.00074 | ms/batch 153.00 | loss 405.95 | gep 204.18 |gepc 200.41 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   7 | 1600/1813 batches | lr 0.00074 | ms/batch 152.60 | loss 406.21 | gep 204.34 |gepc 200.53 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 1700/1813 batches | lr 0.00074 | ms/batch 151.99 | loss 407.15 | gep 204.78 |gepc 201.02 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   7 | 1800/1813 batches | lr 0.00074 | ms/batch 152.47 | loss 405.79 | gep 204.06 |gepc 200.39 |dar  1.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 287.27s | valid loss 204.3241 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   8 | 100/1813 batches | lr 0.00070 | ms/batch 155.12 | loss 409.67 | gep 206.06 |gepc 202.24 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   8 | 200/1813 batches | lr 0.00070 | ms/batch 167.67 | loss 406.49 | gep 204.38 |gepc 200.76 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 300/1813 batches | lr 0.00070 | ms/batch 221.54 | loss 406.51 | gep 204.41 |gepc 200.76 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   8 | 400/1813 batches | lr 0.00070 | ms/batch 227.08 | loss 405.83 | gep 204.16 |gepc 200.32 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 500/1813 batches | lr 0.00070 | ms/batch 194.65 | loss 405.59 | gep 203.93 |gepc 200.31 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   8 | 600/1813 batches | lr 0.00070 | ms/batch 153.22 | loss 405.55 | gep 204.05 |gepc 200.16 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 700/1813 batches | lr 0.00070 | ms/batch 150.90 | loss 405.95 | gep 203.99 |gepc 200.61 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 800/1813 batches | lr 0.00070 | ms/batch 149.35 | loss 406.93 | gep 204.65 |gepc 200.92 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 900/1813 batches | lr 0.00070 | ms/batch 145.30 | loss 406.14 | gep 204.11 |gepc 200.68 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 1000/1813 batches | lr 0.00070 | ms/batch 152.50 | loss 406.03 | gep 204.19 |gepc 200.50 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   8 | 1100/1813 batches | lr 0.00070 | ms/batch 155.27 | loss 404.86 | gep 203.63 |gepc 199.89 |dar  1.33 |\n",
      "scGPT - INFO - | epoch   8 | 1200/1813 batches | lr 0.00070 | ms/batch 148.49 | loss 405.35 | gep 203.89 |gepc 200.12 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 1300/1813 batches | lr 0.00070 | ms/batch 149.70 | loss 406.29 | gep 204.16 |gepc 200.79 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 1400/1813 batches | lr 0.00070 | ms/batch 149.14 | loss 406.91 | gep 204.57 |gepc 200.99 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 1500/1813 batches | lr 0.00070 | ms/batch 149.63 | loss 405.52 | gep 204.00 |gepc 200.18 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   8 | 1600/1813 batches | lr 0.00070 | ms/batch 152.71 | loss 406.22 | gep 204.24 |gepc 200.63 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   8 | 1700/1813 batches | lr 0.00070 | ms/batch 152.61 | loss 406.27 | gep 204.23 |gepc 200.71 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   8 | 1800/1813 batches | lr 0.00070 | ms/batch 150.77 | loss 405.76 | gep 204.04 |gepc 200.36 |dar  1.36 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 304.92s | valid loss 204.1597 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch   9 | 100/1813 batches | lr 0.00066 | ms/batch 155.88 | loss 410.78 | gep 206.61 |gepc 202.80 |dar  1.37 |\n",
      "scGPT - INFO - | epoch   9 | 200/1813 batches | lr 0.00066 | ms/batch 152.69 | loss 406.42 | gep 204.33 |gepc 200.75 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   9 | 300/1813 batches | lr 0.00066 | ms/batch 156.44 | loss 406.16 | gep 204.23 |gepc 200.59 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   9 | 400/1813 batches | lr 0.00066 | ms/batch 151.40 | loss 406.24 | gep 204.31 |gepc 200.59 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   9 | 500/1813 batches | lr 0.00066 | ms/batch 152.50 | loss 406.36 | gep 204.30 |gepc 200.72 |dar  1.33 |\n",
      "scGPT - INFO - | epoch   9 | 600/1813 batches | lr 0.00066 | ms/batch 152.47 | loss 405.36 | gep 203.84 |gepc 200.18 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   9 | 700/1813 batches | lr 0.00066 | ms/batch 148.59 | loss 405.92 | gep 204.22 |gepc 200.35 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   9 | 800/1813 batches | lr 0.00066 | ms/batch 147.63 | loss 405.66 | gep 204.06 |gepc 200.24 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   9 | 900/1813 batches | lr 0.00066 | ms/batch 154.58 | loss 406.08 | gep 204.24 |gepc 200.48 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   9 | 1000/1813 batches | lr 0.00066 | ms/batch 152.35 | loss 405.85 | gep 204.09 |gepc 200.41 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   9 | 1100/1813 batches | lr 0.00066 | ms/batch 147.24 | loss 406.00 | gep 204.08 |gepc 200.58 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   9 | 1200/1813 batches | lr 0.00066 | ms/batch 152.83 | loss 406.51 | gep 204.42 |gepc 200.73 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   9 | 1300/1813 batches | lr 0.00066 | ms/batch 152.58 | loss 405.53 | gep 203.98 |gepc 200.21 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   9 | 1400/1813 batches | lr 0.00066 | ms/batch 152.14 | loss 405.30 | gep 203.89 |gepc 200.05 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   9 | 1500/1813 batches | lr 0.00066 | ms/batch 153.38 | loss 406.33 | gep 204.35 |gepc 200.64 |dar  1.35 |\n",
      "scGPT - INFO - | epoch   9 | 1600/1813 batches | lr 0.00066 | ms/batch 148.17 | loss 406.23 | gep 204.19 |gepc 200.68 |dar  1.36 |\n",
      "scGPT - INFO - | epoch   9 | 1700/1813 batches | lr 0.00066 | ms/batch 152.40 | loss 406.07 | gep 204.13 |gepc 200.60 |dar  1.34 |\n",
      "scGPT - INFO - | epoch   9 | 1800/1813 batches | lr 0.00066 | ms/batch 147.09 | loss 405.53 | gep 203.98 |gepc 200.20 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 297.44s | valid loss 204.3904 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  10 | 100/1813 batches | lr 0.00063 | ms/batch 153.37 | loss 409.47 | gep 205.97 |gepc 202.14 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  10 | 200/1813 batches | lr 0.00063 | ms/batch 156.81 | loss 405.53 | gep 204.08 |gepc 200.11 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  10 | 300/1813 batches | lr 0.00063 | ms/batch 152.93 | loss 405.25 | gep 203.75 |gepc 200.15 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 400/1813 batches | lr 0.00063 | ms/batch 152.52 | loss 405.90 | gep 204.16 |gepc 200.38 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 500/1813 batches | lr 0.00063 | ms/batch 149.22 | loss 406.80 | gep 204.59 |gepc 200.86 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 600/1813 batches | lr 0.00063 | ms/batch 152.42 | loss 405.44 | gep 203.88 |gepc 200.20 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  10 | 700/1813 batches | lr 0.00063 | ms/batch 164.24 | loss 405.55 | gep 203.95 |gepc 200.26 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  10 | 800/1813 batches | lr 0.00063 | ms/batch 149.78 | loss 405.18 | gep 203.82 |gepc 200.01 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 900/1813 batches | lr 0.00063 | ms/batch 148.20 | loss 405.96 | gep 204.30 |gepc 200.31 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 1000/1813 batches | lr 0.00063 | ms/batch 149.82 | loss 406.48 | gep 204.47 |gepc 200.66 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  10 | 1100/1813 batches | lr 0.00063 | ms/batch 145.47 | loss 405.50 | gep 203.91 |gepc 200.24 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  10 | 1200/1813 batches | lr 0.00063 | ms/batch 152.76 | loss 405.42 | gep 204.06 |gepc 200.01 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 1300/1813 batches | lr 0.00063 | ms/batch 156.46 | loss 405.75 | gep 204.08 |gepc 200.33 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  10 | 1400/1813 batches | lr 0.00063 | ms/batch 150.30 | loss 405.56 | gep 203.92 |gepc 200.28 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  10 | 1500/1813 batches | lr 0.00063 | ms/batch 145.28 | loss 405.85 | gep 204.18 |gepc 200.33 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  10 | 1600/1813 batches | lr 0.00063 | ms/batch 151.17 | loss 406.50 | gep 204.46 |gepc 200.70 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 1700/1813 batches | lr 0.00063 | ms/batch 156.95 | loss 405.87 | gep 204.11 |gepc 200.40 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  10 | 1800/1813 batches | lr 0.00063 | ms/batch 150.95 | loss 406.13 | gep 204.14 |gepc 200.64 |dar  1.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 285.99s | valid loss 204.0130 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Saving model to save/dev_SCMBench-Jan17-11-03\n",
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2015/2015 [01:53<00:00, 17.68it/s]\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/scib/metrics/metrics.py:293: DeprecationWarning: Call to deprecated function (or staticmethod) opt_louvain.\n",
      "  res_max, nmi_max, nmi_all = opt_louvain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_embeddings (32231, 512)\n",
      "NMI...\n",
      "ARI...\n",
      "Silhouette score...\n",
      "PC regression...\n",
      "Graph connectivity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO -                                   0\n",
      "NMI_cluster/label          0.026892\n",
      "ARI_cluster/label          0.000498\n",
      "ASW_label                  0.463391\n",
      "ASW_label/batch            0.945075\n",
      "PCR_batch                  0.960100\n",
      "cell_cycle_conservation         NaN\n",
      "isolated_label_F1               NaN\n",
      "isolated_label_silhouette       NaN\n",
      "graph_conn                 0.025350\n",
      "kBET                            NaN\n",
      "iLISI                           NaN\n",
      "cLISI                           NaN\n",
      "hvg_overlap                     NaN\n",
      "trajectory                      NaN\n",
      "scGPT - INFO - Biological Conservation Metrics: \n",
      "ASW (cell-type): 0.4634, graph cLISI: nan, isolated label silhouette: nan, \n",
      "Batch Effect Removal Metrics: \n",
      "PCR_batch: 0.9601, ASW (batch): 0.9451, graph connectivity: 0.0254, graph iLISI: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch  11, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  11 | 100/1813 batches | lr 0.00060 | ms/batch 165.72 | loss 409.82 | gep 206.09 |gepc 202.37 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  11 | 200/1813 batches | lr 0.00060 | ms/batch 144.94 | loss 406.12 | gep 204.25 |gepc 200.52 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 300/1813 batches | lr 0.00060 | ms/batch 148.33 | loss 405.70 | gep 204.08 |gepc 200.27 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 400/1813 batches | lr 0.00060 | ms/batch 148.04 | loss 404.96 | gep 203.82 |gepc 199.79 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 500/1813 batches | lr 0.00060 | ms/batch 147.37 | loss 405.49 | gep 204.06 |gepc 200.10 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  11 | 600/1813 batches | lr 0.00060 | ms/batch 150.80 | loss 406.00 | gep 204.20 |gepc 200.46 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  11 | 700/1813 batches | lr 0.00060 | ms/batch 153.09 | loss 405.93 | gep 204.12 |gepc 200.46 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 800/1813 batches | lr 0.00060 | ms/batch 149.54 | loss 406.74 | gep 204.58 |gepc 200.81 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  11 | 900/1813 batches | lr 0.00060 | ms/batch 148.47 | loss 405.64 | gep 204.07 |gepc 200.24 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  11 | 1000/1813 batches | lr 0.00060 | ms/batch 146.72 | loss 406.67 | gep 204.59 |gepc 200.74 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  11 | 1100/1813 batches | lr 0.00060 | ms/batch 144.88 | loss 406.20 | gep 204.39 |gepc 200.46 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  11 | 1200/1813 batches | lr 0.00060 | ms/batch 144.84 | loss 405.44 | gep 203.94 |gepc 200.16 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 1300/1813 batches | lr 0.00060 | ms/batch 146.42 | loss 405.72 | gep 204.11 |gepc 200.26 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 1400/1813 batches | lr 0.00060 | ms/batch 146.79 | loss 406.31 | gep 204.31 |gepc 200.65 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  11 | 1500/1813 batches | lr 0.00060 | ms/batch 148.16 | loss 406.40 | gep 204.45 |gepc 200.60 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 1600/1813 batches | lr 0.00060 | ms/batch 147.59 | loss 406.37 | gep 204.44 |gepc 200.58 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  11 | 1700/1813 batches | lr 0.00060 | ms/batch 149.09 | loss 405.33 | gep 203.90 |gepc 200.08 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  11 | 1800/1813 batches | lr 0.00060 | ms/batch 155.60 | loss 406.75 | gep 204.54 |gepc 200.85 |dar  1.36 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 280.77s | valid loss 204.2962 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  12 | 100/1813 batches | lr 0.00057 | ms/batch 158.49 | loss 410.25 | gep 206.35 |gepc 202.54 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  12 | 200/1813 batches | lr 0.00057 | ms/batch 149.06 | loss 405.92 | gep 204.10 |gepc 200.47 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 300/1813 batches | lr 0.00057 | ms/batch 151.80 | loss 405.49 | gep 204.11 |gepc 200.04 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 400/1813 batches | lr 0.00057 | ms/batch 148.26 | loss 405.54 | gep 204.10 |gepc 200.08 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  12 | 500/1813 batches | lr 0.00057 | ms/batch 151.59 | loss 406.38 | gep 204.35 |gepc 200.68 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 600/1813 batches | lr 0.00057 | ms/batch 150.34 | loss 405.73 | gep 204.07 |gepc 200.31 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 700/1813 batches | lr 0.00057 | ms/batch 146.85 | loss 406.48 | gep 204.57 |gepc 200.55 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 800/1813 batches | lr 0.00057 | ms/batch 149.89 | loss 406.16 | gep 204.35 |gepc 200.47 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 900/1813 batches | lr 0.00057 | ms/batch 150.83 | loss 406.07 | gep 204.26 |gepc 200.47 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  12 | 1000/1813 batches | lr 0.00057 | ms/batch 151.08 | loss 405.91 | gep 204.24 |gepc 200.31 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  12 | 1100/1813 batches | lr 0.00057 | ms/batch 145.57 | loss 406.69 | gep 204.47 |gepc 200.89 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  12 | 1200/1813 batches | lr 0.00057 | ms/batch 148.83 | loss 405.77 | gep 204.16 |gepc 200.28 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  12 | 1300/1813 batches | lr 0.00057 | ms/batch 149.12 | loss 405.76 | gep 204.12 |gepc 200.30 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 1400/1813 batches | lr 0.00057 | ms/batch 152.97 | loss 406.10 | gep 204.20 |gepc 200.54 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 1500/1813 batches | lr 0.00057 | ms/batch 151.29 | loss 406.49 | gep 204.39 |gepc 200.75 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  12 | 1600/1813 batches | lr 0.00057 | ms/batch 148.58 | loss 405.22 | gep 203.87 |gepc 200.00 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  12 | 1700/1813 batches | lr 0.00057 | ms/batch 148.56 | loss 405.62 | gep 204.07 |gepc 200.22 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  12 | 1800/1813 batches | lr 0.00057 | ms/batch 152.09 | loss 405.00 | gep 203.76 |gepc 199.89 |dar  1.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 281.74s | valid loss 204.1582 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  13 | 100/1813 batches | lr 0.00054 | ms/batch 147.68 | loss 410.02 | gep 206.38 |gepc 202.28 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  13 | 200/1813 batches | lr 0.00054 | ms/batch 147.80 | loss 405.12 | gep 203.82 |gepc 199.96 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  13 | 300/1813 batches | lr 0.00054 | ms/batch 147.13 | loss 405.72 | gep 204.06 |gepc 200.33 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  13 | 400/1813 batches | lr 0.00054 | ms/batch 148.16 | loss 405.13 | gep 203.85 |gepc 199.94 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  13 | 500/1813 batches | lr 0.00054 | ms/batch 152.87 | loss 405.56 | gep 204.00 |gepc 200.22 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  13 | 600/1813 batches | lr 0.00054 | ms/batch 147.81 | loss 405.94 | gep 204.16 |gepc 200.43 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  13 | 700/1813 batches | lr 0.00054 | ms/batch 148.32 | loss 405.34 | gep 203.95 |gepc 200.06 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  13 | 800/1813 batches | lr 0.00054 | ms/batch 149.39 | loss 405.93 | gep 204.25 |gepc 200.33 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  13 | 900/1813 batches | lr 0.00054 | ms/batch 150.17 | loss 405.34 | gep 204.03 |gepc 199.96 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  13 | 1000/1813 batches | lr 0.00054 | ms/batch 152.52 | loss 407.21 | gep 205.07 |gepc 200.77 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  13 | 1100/1813 batches | lr 0.00054 | ms/batch 149.38 | loss 405.89 | gep 204.13 |gepc 200.41 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  13 | 1200/1813 batches | lr 0.00054 | ms/batch 144.54 | loss 405.14 | gep 203.95 |gepc 199.85 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  13 | 1300/1813 batches | lr 0.00054 | ms/batch 140.68 | loss 405.53 | gep 204.07 |gepc 200.10 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  13 | 1400/1813 batches | lr 0.00054 | ms/batch 145.31 | loss 405.45 | gep 203.97 |gepc 200.14 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  13 | 1500/1813 batches | lr 0.00054 | ms/batch 146.60 | loss 406.31 | gep 204.52 |gepc 200.44 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  13 | 1600/1813 batches | lr 0.00054 | ms/batch 144.32 | loss 406.08 | gep 204.21 |gepc 200.52 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  13 | 1700/1813 batches | lr 0.00054 | ms/batch 155.58 | loss 405.78 | gep 204.06 |gepc 200.37 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  13 | 1800/1813 batches | lr 0.00054 | ms/batch 153.57 | loss 406.02 | gep 204.28 |gepc 200.38 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 278.29s | valid loss 204.3995 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  14 | 100/1813 batches | lr 0.00051 | ms/batch 148.69 | loss 410.00 | gep 206.23 |gepc 202.41 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  14 | 200/1813 batches | lr 0.00051 | ms/batch 149.99 | loss 405.72 | gep 204.16 |gepc 200.22 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 300/1813 batches | lr 0.00051 | ms/batch 150.46 | loss 405.51 | gep 204.05 |gepc 200.11 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 400/1813 batches | lr 0.00051 | ms/batch 150.13 | loss 405.44 | gep 204.07 |gepc 200.03 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 500/1813 batches | lr 0.00051 | ms/batch 153.45 | loss 405.81 | gep 204.25 |gepc 200.22 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  14 | 600/1813 batches | lr 0.00051 | ms/batch 158.74 | loss 405.37 | gep 204.01 |gepc 200.01 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  14 | 700/1813 batches | lr 0.00051 | ms/batch 153.51 | loss 405.19 | gep 203.92 |gepc 199.93 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  14 | 800/1813 batches | lr 0.00051 | ms/batch 145.37 | loss 405.92 | gep 204.28 |gepc 200.28 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 900/1813 batches | lr 0.00051 | ms/batch 145.11 | loss 406.28 | gep 204.43 |gepc 200.50 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 1000/1813 batches | lr 0.00051 | ms/batch 147.01 | loss 406.49 | gep 204.56 |gepc 200.59 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  14 | 1100/1813 batches | lr 0.00051 | ms/batch 157.37 | loss 406.56 | gep 204.52 |gepc 200.70 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  14 | 1200/1813 batches | lr 0.00051 | ms/batch 148.34 | loss 405.29 | gep 203.96 |gepc 199.98 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  14 | 1300/1813 batches | lr 0.00051 | ms/batch 149.40 | loss 405.23 | gep 203.83 |gepc 200.07 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  14 | 1400/1813 batches | lr 0.00051 | ms/batch 151.88 | loss 405.24 | gep 203.83 |gepc 200.05 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 1500/1813 batches | lr 0.00051 | ms/batch 157.90 | loss 405.61 | gep 204.02 |gepc 200.23 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  14 | 1600/1813 batches | lr 0.00051 | ms/batch 149.54 | loss 406.13 | gep 204.29 |gepc 200.49 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 1700/1813 batches | lr 0.00051 | ms/batch 145.83 | loss 405.59 | gep 204.09 |gepc 200.15 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  14 | 1800/1813 batches | lr 0.00051 | ms/batch 146.09 | loss 406.08 | gep 204.29 |gepc 200.44 |dar  1.36 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 281.95s | valid loss 204.2808 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  15 | 100/1813 batches | lr 0.00049 | ms/batch 152.76 | loss 409.02 | gep 205.80 |gepc 201.86 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  15 | 200/1813 batches | lr 0.00049 | ms/batch 140.00 | loss 406.38 | gep 204.52 |gepc 200.51 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 300/1813 batches | lr 0.00049 | ms/batch 144.33 | loss 406.07 | gep 204.37 |gepc 200.36 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  15 | 400/1813 batches | lr 0.00049 | ms/batch 150.09 | loss 405.24 | gep 203.98 |gepc 199.91 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 500/1813 batches | lr 0.00049 | ms/batch 149.08 | loss 405.06 | gep 203.79 |gepc 199.93 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  15 | 600/1813 batches | lr 0.00049 | ms/batch 151.92 | loss 405.79 | gep 204.19 |gepc 200.25 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  15 | 700/1813 batches | lr 0.00049 | ms/batch 146.99 | loss 405.70 | gep 204.19 |gepc 200.17 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 800/1813 batches | lr 0.00049 | ms/batch 150.10 | loss 405.90 | gep 204.31 |gepc 200.24 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 900/1813 batches | lr 0.00049 | ms/batch 148.23 | loss 405.28 | gep 203.99 |gepc 199.94 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 1000/1813 batches | lr 0.00049 | ms/batch 147.70 | loss 405.70 | gep 204.08 |gepc 200.28 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  15 | 1100/1813 batches | lr 0.00049 | ms/batch 150.41 | loss 406.01 | gep 204.25 |gepc 200.42 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  15 | 1200/1813 batches | lr 0.00049 | ms/batch 149.10 | loss 405.76 | gep 204.22 |gepc 200.18 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 1300/1813 batches | lr 0.00049 | ms/batch 147.49 | loss 406.40 | gep 204.45 |gepc 200.59 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 1400/1813 batches | lr 0.00049 | ms/batch 148.06 | loss 405.59 | gep 204.18 |gepc 200.07 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  15 | 1500/1813 batches | lr 0.00049 | ms/batch 148.64 | loss 405.42 | gep 204.06 |gepc 200.01 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 1600/1813 batches | lr 0.00049 | ms/batch 151.12 | loss 406.42 | gep 204.49 |gepc 200.59 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 1700/1813 batches | lr 0.00049 | ms/batch 150.16 | loss 405.04 | gep 203.85 |gepc 199.84 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  15 | 1800/1813 batches | lr 0.00049 | ms/batch 142.75 | loss 404.76 | gep 203.71 |gepc 199.70 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 277.98s | valid loss 204.1458 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Saving model to save/dev_SCMBench-Jan17-11-03\n",
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2015/2015 [01:52<00:00, 17.86it/s]\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/scib/metrics/metrics.py:293: DeprecationWarning: Call to deprecated function (or staticmethod) opt_louvain.\n",
      "  res_max, nmi_max, nmi_all = opt_louvain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_embeddings (32231, 512)\n",
      "NMI...\n",
      "ARI...\n",
      "Silhouette score...\n",
      "PC regression...\n",
      "Graph connectivity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO -                                   0\n",
      "NMI_cluster/label          0.026829\n",
      "ARI_cluster/label          0.000722\n",
      "ASW_label                  0.464137\n",
      "ASW_label/batch            0.944730\n",
      "PCR_batch                  0.958833\n",
      "cell_cycle_conservation         NaN\n",
      "isolated_label_F1               NaN\n",
      "isolated_label_silhouette       NaN\n",
      "graph_conn                 0.024838\n",
      "kBET                            NaN\n",
      "iLISI                           NaN\n",
      "cLISI                           NaN\n",
      "hvg_overlap                     NaN\n",
      "trajectory                      NaN\n",
      "scGPT - INFO - Biological Conservation Metrics: \n",
      "ASW (cell-type): 0.4641, graph cLISI: nan, isolated label silhouette: nan, \n",
      "Batch Effect Removal Metrics: \n",
      "PCR_batch: 0.9588, ASW (batch): 0.9447, graph connectivity: 0.0248, graph iLISI: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch  16, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  16 | 100/1813 batches | lr 0.00046 | ms/batch 146.86 | loss 408.88 | gep 205.76 |gepc 201.76 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  16 | 200/1813 batches | lr 0.00046 | ms/batch 145.70 | loss 406.62 | gep 204.60 |gepc 200.69 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  16 | 300/1813 batches | lr 0.00046 | ms/batch 145.90 | loss 405.34 | gep 203.99 |gepc 200.00 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 400/1813 batches | lr 0.00046 | ms/batch 145.61 | loss 406.23 | gep 204.47 |gepc 200.41 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 500/1813 batches | lr 0.00046 | ms/batch 146.58 | loss 404.99 | gep 203.79 |gepc 199.87 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  16 | 600/1813 batches | lr 0.00046 | ms/batch 149.75 | loss 405.54 | gep 204.15 |gepc 200.03 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  16 | 700/1813 batches | lr 0.00046 | ms/batch 146.00 | loss 404.85 | gep 203.75 |gepc 199.76 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  16 | 800/1813 batches | lr 0.00046 | ms/batch 150.09 | loss 405.98 | gep 204.31 |gepc 200.32 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 900/1813 batches | lr 0.00046 | ms/batch 151.84 | loss 406.45 | gep 204.56 |gepc 200.53 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  16 | 1000/1813 batches | lr 0.00046 | ms/batch 151.13 | loss 405.27 | gep 204.09 |gepc 199.83 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  16 | 1100/1813 batches | lr 0.00046 | ms/batch 149.63 | loss 406.12 | gep 204.39 |gepc 200.38 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 1200/1813 batches | lr 0.00046 | ms/batch 149.85 | loss 405.05 | gep 203.83 |gepc 199.87 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 1300/1813 batches | lr 0.00046 | ms/batch 148.30 | loss 405.82 | gep 204.21 |gepc 200.27 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 1400/1813 batches | lr 0.00046 | ms/batch 140.92 | loss 406.91 | gep 204.80 |gepc 200.77 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 1500/1813 batches | lr 0.00046 | ms/batch 138.03 | loss 404.40 | gep 203.40 |gepc 199.66 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  16 | 1600/1813 batches | lr 0.00046 | ms/batch 139.04 | loss 406.04 | gep 204.28 |gepc 200.43 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  16 | 1700/1813 batches | lr 0.00046 | ms/batch 147.01 | loss 405.48 | gep 204.14 |gepc 200.00 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  16 | 1800/1813 batches | lr 0.00046 | ms/batch 146.78 | loss 405.91 | gep 204.27 |gepc 200.29 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  16 | time: 275.56s | valid loss 204.1848 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  17, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  17 | 100/1813 batches | lr 0.00044 | ms/batch 148.36 | loss 409.39 | gep 206.07 |gepc 201.97 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 200/1813 batches | lr 0.00044 | ms/batch 147.82 | loss 406.15 | gep 204.44 |gepc 200.37 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  17 | 300/1813 batches | lr 0.00044 | ms/batch 143.86 | loss 406.24 | gep 204.49 |gepc 200.40 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  17 | 400/1813 batches | lr 0.00044 | ms/batch 149.67 | loss 406.12 | gep 204.43 |gepc 200.35 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 500/1813 batches | lr 0.00044 | ms/batch 148.66 | loss 405.37 | gep 204.10 |gepc 199.94 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  17 | 600/1813 batches | lr 0.00044 | ms/batch 147.19 | loss 404.66 | gep 203.60 |gepc 199.72 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  17 | 700/1813 batches | lr 0.00044 | ms/batch 147.26 | loss 405.78 | gep 204.32 |gepc 200.12 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 800/1813 batches | lr 0.00044 | ms/batch 146.57 | loss 405.82 | gep 204.26 |gepc 200.22 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  17 | 900/1813 batches | lr 0.00044 | ms/batch 146.31 | loss 405.53 | gep 204.13 |gepc 200.04 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 1000/1813 batches | lr 0.00044 | ms/batch 146.92 | loss 405.92 | gep 204.41 |gepc 200.16 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 1100/1813 batches | lr 0.00044 | ms/batch 147.37 | loss 405.68 | gep 204.24 |gepc 200.10 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 1200/1813 batches | lr 0.00044 | ms/batch 148.47 | loss 405.40 | gep 204.05 |gepc 200.00 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 1300/1813 batches | lr 0.00044 | ms/batch 145.41 | loss 405.59 | gep 204.09 |gepc 200.15 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 1400/1813 batches | lr 0.00044 | ms/batch 146.07 | loss 405.22 | gep 203.90 |gepc 199.96 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  17 | 1500/1813 batches | lr 0.00044 | ms/batch 147.00 | loss 405.58 | gep 204.07 |gepc 200.15 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  17 | 1600/1813 batches | lr 0.00044 | ms/batch 147.70 | loss 406.21 | gep 204.38 |gepc 200.50 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  17 | 1700/1813 batches | lr 0.00044 | ms/batch 150.07 | loss 405.10 | gep 203.81 |gepc 199.95 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  17 | 1800/1813 batches | lr 0.00044 | ms/batch 147.63 | loss 406.39 | gep 204.46 |gepc 200.57 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  17 | time: 276.29s | valid loss 203.9937 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  18, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  18 | 100/1813 batches | lr 0.00042 | ms/batch 150.81 | loss 409.69 | gep 206.18 |gepc 202.16 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  18 | 200/1813 batches | lr 0.00042 | ms/batch 150.12 | loss 405.03 | gep 203.84 |gepc 199.86 |dar  1.33 |\n",
      "scGPT - INFO - | epoch  18 | 300/1813 batches | lr 0.00042 | ms/batch 146.70 | loss 405.19 | gep 203.91 |gepc 199.92 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  18 | 400/1813 batches | lr 0.00042 | ms/batch 145.91 | loss 405.33 | gep 203.98 |gepc 200.01 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  18 | 500/1813 batches | lr 0.00042 | ms/batch 146.72 | loss 404.82 | gep 203.83 |gepc 199.65 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  18 | 600/1813 batches | lr 0.00042 | ms/batch 148.63 | loss 406.03 | gep 204.35 |gepc 200.33 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  18 | 700/1813 batches | lr 0.00042 | ms/batch 147.32 | loss 405.52 | gep 204.14 |gepc 200.03 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  18 | 800/1813 batches | lr 0.00042 | ms/batch 145.46 | loss 405.80 | gep 204.27 |gepc 200.18 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  18 | 900/1813 batches | lr 0.00042 | ms/batch 149.89 | loss 405.44 | gep 204.18 |gepc 199.91 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  18 | 1000/1813 batches | lr 0.00042 | ms/batch 149.65 | loss 405.38 | gep 204.09 |gepc 199.95 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  18 | 1100/1813 batches | lr 0.00042 | ms/batch 146.82 | loss 405.66 | gep 204.20 |gepc 200.11 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  18 | 1200/1813 batches | lr 0.00042 | ms/batch 148.39 | loss 404.85 | gep 203.82 |gepc 199.68 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  18 | 1300/1813 batches | lr 0.00042 | ms/batch 145.26 | loss 405.07 | gep 203.83 |gepc 199.89 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  18 | 1400/1813 batches | lr 0.00042 | ms/batch 149.77 | loss 405.64 | gep 204.19 |gepc 200.10 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  18 | 1500/1813 batches | lr 0.00042 | ms/batch 150.54 | loss 405.14 | gep 203.91 |gepc 199.88 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  18 | 1600/1813 batches | lr 0.00042 | ms/batch 149.56 | loss 405.37 | gep 204.03 |gepc 200.00 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  18 | 1700/1813 batches | lr 0.00042 | ms/batch 149.30 | loss 404.81 | gep 203.72 |gepc 199.74 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  18 | 1800/1813 batches | lr 0.00042 | ms/batch 147.91 | loss 405.78 | gep 204.21 |gepc 200.23 |dar  1.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  18 | time: 277.86s | valid loss 204.0249 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  19, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  19 | 100/1813 batches | lr 0.00040 | ms/batch 151.62 | loss 409.79 | gep 206.34 |gepc 202.10 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 200/1813 batches | lr 0.00040 | ms/batch 149.31 | loss 405.51 | gep 204.16 |gepc 200.00 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 300/1813 batches | lr 0.00040 | ms/batch 144.62 | loss 405.52 | gep 204.12 |gepc 200.04 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  19 | 400/1813 batches | lr 0.00040 | ms/batch 155.93 | loss 405.44 | gep 204.15 |gepc 199.95 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  19 | 500/1813 batches | lr 0.00040 | ms/batch 145.94 | loss 405.56 | gep 204.11 |gepc 200.10 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 600/1813 batches | lr 0.00040 | ms/batch 146.19 | loss 405.43 | gep 204.16 |gepc 199.93 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  19 | 700/1813 batches | lr 0.00040 | ms/batch 149.59 | loss 405.60 | gep 204.20 |gepc 200.05 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  19 | 800/1813 batches | lr 0.00040 | ms/batch 151.19 | loss 405.38 | gep 204.17 |gepc 199.86 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 900/1813 batches | lr 0.00040 | ms/batch 149.36 | loss 405.41 | gep 204.17 |gepc 199.89 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 1000/1813 batches | lr 0.00040 | ms/batch 146.92 | loss 404.89 | gep 203.85 |gepc 199.70 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 1100/1813 batches | lr 0.00040 | ms/batch 148.01 | loss 405.26 | gep 204.10 |gepc 199.81 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 1200/1813 batches | lr 0.00040 | ms/batch 148.24 | loss 406.19 | gep 204.63 |gepc 200.22 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  19 | 1300/1813 batches | lr 0.00040 | ms/batch 155.40 | loss 405.67 | gep 204.26 |gepc 200.05 |dar  1.36 |\n",
      "scGPT - INFO - | epoch  19 | 1400/1813 batches | lr 0.00040 | ms/batch 150.23 | loss 405.89 | gep 204.29 |gepc 200.25 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 1500/1813 batches | lr 0.00040 | ms/batch 146.43 | loss 405.17 | gep 203.99 |gepc 199.82 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 1600/1813 batches | lr 0.00040 | ms/batch 148.47 | loss 406.01 | gep 204.33 |gepc 200.34 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  19 | 1700/1813 batches | lr 0.00040 | ms/batch 147.98 | loss 406.90 | gep 204.84 |gepc 200.72 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  19 | 1800/1813 batches | lr 0.00040 | ms/batch 146.64 | loss 405.42 | gep 204.07 |gepc 200.02 |dar  1.33 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  19 | time: 279.33s | valid loss 203.9057 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  20, ratio of masked values in train:  0.3995\n",
      "scGPT - INFO - | epoch  20 | 100/1813 batches | lr 0.00038 | ms/batch 148.10 | loss 409.87 | gep 206.31 |gepc 202.19 |dar  1.37 |\n",
      "scGPT - INFO - | epoch  20 | 200/1813 batches | lr 0.00038 | ms/batch 146.48 | loss 405.67 | gep 204.18 |gepc 200.15 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 300/1813 batches | lr 0.00038 | ms/batch 149.07 | loss 406.24 | gep 204.46 |gepc 200.44 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 400/1813 batches | lr 0.00038 | ms/batch 150.95 | loss 404.81 | gep 203.92 |gepc 199.54 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 500/1813 batches | lr 0.00038 | ms/batch 149.87 | loss 404.93 | gep 203.86 |gepc 199.72 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 600/1813 batches | lr 0.00038 | ms/batch 155.00 | loss 404.86 | gep 203.88 |gepc 199.63 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 700/1813 batches | lr 0.00038 | ms/batch 165.77 | loss 405.11 | gep 204.00 |gepc 199.77 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 800/1813 batches | lr 0.00038 | ms/batch 153.40 | loss 405.35 | gep 204.14 |gepc 199.85 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 900/1813 batches | lr 0.00038 | ms/batch 156.26 | loss 404.96 | gep 203.83 |gepc 199.79 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 1000/1813 batches | lr 0.00038 | ms/batch 150.45 | loss 405.76 | gep 204.27 |gepc 200.14 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 1100/1813 batches | lr 0.00038 | ms/batch 152.97 | loss 405.62 | gep 204.18 |gepc 200.09 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 1200/1813 batches | lr 0.00038 | ms/batch 154.28 | loss 405.07 | gep 203.95 |gepc 199.77 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 1300/1813 batches | lr 0.00038 | ms/batch 153.31 | loss 405.75 | gep 204.24 |gepc 200.16 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 1400/1813 batches | lr 0.00038 | ms/batch 159.91 | loss 405.53 | gep 204.19 |gepc 200.00 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 1500/1813 batches | lr 0.00038 | ms/batch 160.75 | loss 405.89 | gep 204.35 |gepc 200.20 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 1600/1813 batches | lr 0.00038 | ms/batch 158.32 | loss 405.09 | gep 203.95 |gepc 199.80 |dar  1.34 |\n",
      "scGPT - INFO - | epoch  20 | 1700/1813 batches | lr 0.00038 | ms/batch 150.18 | loss 405.07 | gep 203.86 |gepc 199.86 |dar  1.35 |\n",
      "scGPT - INFO - | epoch  20 | 1800/1813 batches | lr 0.00038 | ms/batch 159.13 | loss 405.91 | gep 204.30 |gepc 200.27 |dar  1.35 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  20 | time: 289.35s | valid loss 203.9740 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Saving model to save/dev_SCMBench-Jan17-11-03\n",
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2015/2015 [01:55<00:00, 17.40it/s]\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/scib/metrics/metrics.py:293: DeprecationWarning: Call to deprecated function (or staticmethod) opt_louvain.\n",
      "  res_max, nmi_max, nmi_all = opt_louvain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_embeddings (32231, 512)\n",
      "NMI...\n",
      "ARI...\n",
      "Silhouette score...\n",
      "PC regression...\n",
      "Graph connectivity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO -                                   0\n",
      "NMI_cluster/label          0.026546\n",
      "ARI_cluster/label          0.000615\n",
      "ASW_label                  0.462839\n",
      "ASW_label/batch            0.945133\n",
      "PCR_batch                  0.958912\n",
      "cell_cycle_conservation         NaN\n",
      "isolated_label_F1               NaN\n",
      "isolated_label_silhouette       NaN\n",
      "graph_conn                 0.026620\n",
      "kBET                            NaN\n",
      "iLISI                           NaN\n",
      "cLISI                           NaN\n",
      "hvg_overlap                     NaN\n",
      "trajectory                      NaN\n",
      "scGPT - INFO - Biological Conservation Metrics: \n",
      "ASW (cell-type): 0.4628, graph cLISI: nan, isolated label silhouette: nan, \n",
      "Batch Effect Removal Metrics: \n",
      "PCR_batch: 0.9589, ASW (batch): 0.9451, graph connectivity: 0.0266, graph iLISI: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/pandas/core/algorithms.py:522: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(\n",
    "        tokenized_train=tokenized_train, \n",
    "        tokenized_valid=tokenized_valid, \n",
    "        train_batch_labels=train_batch_labels,\n",
    "        valid_batch_labels=valid_batch_labels,\n",
    "        config=config,\n",
    "        epoch=epoch,\n",
    "        sort_seq_batch=config.per_seq_batch_sample)\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "        per_seq_batch_sample=config.per_seq_batch_sample\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "        per_seq_batch_sample=config.per_seq_batch_sample\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            vocab=vocab,\n",
    "            criterion_gep_gepc=criterion_gep_gepc if config.GEP and config.GEPC else None,\n",
    "            criterion_dab=criterion_dab if config.DAR else None,\n",
    "            criterion_cls=criterion_cls if config.CLS else None,\n",
    "            scaler=scaler,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            logger=logger,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    val_loss = evaluate(\n",
    "        model=model,\n",
    "        loader=valid_loader,\n",
    "        vocab=vocab,\n",
    "        criterion_gep_gepc=criterion_gep_gepc if config.GEP and config.GEPC else None,\n",
    "        criterion_dab=criterion_dab if config.DAR else None,\n",
    "        criterion_cls=criterion_cls if config.CLS else None,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss {val_loss:5.4f} | \"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        logger.info(f\"Saving model to {save_dir}\")\n",
    "        torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "        # eval on testdata\n",
    "        # results = eval_testdata(\n",
    "        #     model = best_model,\n",
    "        #     adata_t = adata_sorted if config.per_seq_batch_sample else adata,\n",
    "        #     gene_ids = gene_ids,\n",
    "        #     vocab = vocab,\n",
    "        #     config = config,\n",
    "        #     logger = logger,\n",
    "        #     include_types=[\"cls\"],\n",
    "        # )\n",
    "        results,cell_embeddings = eval_testdata_save_embed(\n",
    "            model = best_model,\n",
    "            adata_t = adata_sorted if config.per_seq_batch_sample else adata,\n",
    "            gene_ids = gene_ids,\n",
    "            vocab = vocab,\n",
    "            config = config,\n",
    "            logger = logger,\n",
    "            include_types=[\"cls\"],\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(cell_embeddings, index=adata.obs_names).to_csv(output_rna_dir, header=False)\n",
    "        pd.DataFrame(cell_embeddings, index=adata.obs_names).to_csv(output_atac_dir, header=False)\n",
    "\n",
    "        results[\"batch_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "        metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "        metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "\n",
    "        metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "        metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "        wandb.log(metrics_to_log)\n",
    "        wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30430479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f160cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:89: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af63bd034d045758300f65a70f5ed70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.030 MB of 214.105 MB uploaded\\r'), FloatProgress(value=0.009481620830042203, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n",
      "/mnt/nas/user/yixuan/miniconda3/envs/scGPT/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_bio</td><td>▆█▇█▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>test/ARI_cluster/label</td><td>▁▃▅▅█</td></tr><tr><td>test/ASW_label</td><td>▅█▂▃▁</td></tr><tr><td>test/ASW_label/batch</td><td>▁▅▇█▅</td></tr><tr><td>test/NMI_cluster/label</td><td>▆▁██▁</td></tr><tr><td>test/PCR_batch</td><td>▄▁█▆█</td></tr><tr><td>test/avg_bio</td><td>▆█▇█▁</td></tr><tr><td>test/best_model_epoch</td><td>▁▁▁▁▁</td></tr><tr><td>test/graph_conn</td><td>▄▃▁█▂</td></tr><tr><td>train/dab</td><td>▁█▄▄▄▃▃▄▃▃▃▃▃▃▃▂▂▃▃▃▄▃▃▂▃▄▂▃▃▃▂▃▂▂▂▄▂▂▂▃</td></tr><tr><td>train/gep</td><td>█▅█▇▅▄▅▄▅▄▄▄▄▅▅▅▃▄▃▅▅▃▄▄▅▄▄▄▅▄▂▄▄▄▃▁▂▄▂▁</td></tr><tr><td>train/mvc</td><td>█▄▆▅▄▃▄▃▄▄▃▃▄▅▄▄▃▃▃▄▅▃▃▃▄▃▃▃▄▃▂▃▃▃▃▁▂▃▂▁</td></tr><tr><td>valid/loss</td><td>▇▆▃▁▃▄▃▂▂▂▂▂▂▃▃▂▂▃▃▃▄▅█▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_bio</td><td>0.16717</td></tr><tr><td>epoch</td><td>25</td></tr><tr><td>test/ARI_cluster/label</td><td>0.00884</td></tr><tr><td>test/ASW_label</td><td>0.44705</td></tr><tr><td>test/ASW_label/batch</td><td>0.90225</td></tr><tr><td>test/NMI_cluster/label</td><td>0.04563</td></tr><tr><td>test/PCR_batch</td><td>0.85152</td></tr><tr><td>test/best_model_epoch</td><td>4</td></tr><tr><td>test/graph_conn</td><td>0.11607</td></tr><tr><td>train/dab</td><td>1.37073</td></tr><tr><td>train/gep</td><td>190.33325</td></tr><tr><td>train/mvc</td><td>191.30891</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-plasma-47</strong> at: <a href='https://wandb.ai/18763105182/scGPT/runs/46zo0it8' target=\"_blank\">https://wandb.ai/18763105182/scGPT/runs/46zo0it8</a><br/>Synced 6 W&B file(s), 10 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240116_232603-46zo0it8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "99024"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "glob_str = os.path.join(save_dir, \"best_model.pt\")\n",
    "artifact.add_file(glob_str)\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "run.finish()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5bf54-0d47-44da-a34b-4df3fac76124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b6334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03cafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
